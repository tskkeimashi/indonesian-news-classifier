# -*- coding: utf-8 -*-
"""Submission_Klasifikasi Berita

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y9ZOmg322oNegUY2z8Mrgq1wF8hDn6wN
"""

import pandas as pd
import pickle

datanya = pickle.load(open('training.res', 'rb'))
df = pd.DataFrame({'content':datanya[0], 'label':datanya[1]})
print(df.shape)
print(df.head())

print(df['label'].value_counts())

import seaborn as sns

sns.countplot(df['label'])

import re
import nltk
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory


space = re.compile('[/(){}\[\]\|@,;]')
symbols = re.compile('[^0-9a-z #+_]')

stop_factory = StopWordRemoverFactory()
more_stopword = ['dengan', 'ia','bahwa','oleh']
data = set(stop_factory.get_stop_words()+more_stopword)
stopword = stop_factory.create_stop_word_remover()

def clean_text(text):
  text = text.lower()
  text = space.sub(' ', text)
  text = symbols.sub(' ', text)
  text = text.replace('x', '')
  text = ' '.join(word for word in text.split() if word not in data)
  return text

df['content']=df['content'].apply(clean_text)

print(df['content'][0])

kategori = pd.get_dummies(df.label)
df = pd.concat([df, kategori], axis=1)
df = df.drop(columns=['label'])
print(df.head())
print(df.shape)

g=[]
for i in df['content']:
  g.append(i)
panjangmaks = max([len(s) for s in g])
print(panjangmaks) # max length dari padding

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=50000, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(df['content'].values)
word_index = tokenizer.word_index
print('token unik %s' % len(word_index))
print(df.columns)

from keras.preprocessing.sequence import pad_sequences

x= tokenizer.texts_to_sequences(df['content'].values)
x = pad_sequences(x, maxlen = 1000)
y = df[['bisnis', 'bola', 'news', 'otomotif', 'tekno']].values
print(x.shape)
print(y.shape)

df.head()

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(50000, output_dim=16, input_length=1000),
    tf.keras.layers.SpatialDropout1D(0.2),
    tf.keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

from keras.callbacks import EarlyStopping

history = model.fit(x_train, y_train, epochs=10, callbacks=[EarlyStopping(monitor='val_accuracy', mode='max', patience=0)], batch_size=64, validation_split=0.2, validation_data=(x_test, y_test), verbose=1)

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('akurasi')
plt.xlabel('epoch')
plt.legend(['train','test'], loc = 'upper left')
plt.show()

import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc = 'upper left')
plt.show()